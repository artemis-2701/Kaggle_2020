{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt \nplt.rc(\"font\", size=14)\n\nimport seaborn as sns\nsns.set(style=\"white\") #white background style for seaborn plots\nsns.set(style=\"whitegrid\", color_codes=True)\nimport matplotlib.pyplot as plt\n\n\nimport warnings\nwarnings.simplefilter(action='ignore')\n\n#classification model\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\n\n\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"test_data=pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntrain_data=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ngender_submission=pd.read_csv(\"/kaggle/input/titanic/gender_submission.csv\")\n\n#print(test_data)\ntrain_data.head()\n#print(gender_submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The number of samples into the train data is {}.'.format(train_data.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing percentage of age info\nprint(\"% of missing age\")\nprint(round(train_data['Age'].isnull().sum()/train_data.shape[0]*100,2))\n\n\n#visualizing distribution of age\nax = train_data[\"Age\"].hist(bins=15, density=True, stacked=True, color='teal', alpha=0.6)\ntrain_data[\"Age\"].plot(kind='density', color='blue')\nax.set(xlabel='Age')\nplt.xlim(0,85)\nplt.show()\n\n\n#mean age\nmean_age=round(train_data['Age'].sum()/(train_data.shape[0]-train_data['Age'].isnull().sum()),2)\nmedian_age=train_data['Age'].median(skipna=True)\nprint(\"mean age \",mean_age)\nprint(\"median age \",median_age)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing percentage of cabin info\nprint(\"% of missing cabin info\")\nprint(round(train_data['Cabin'].isnull().sum()/train_data.shape[0]*100,2))\nprint('we should ignore this feature for our model')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#embarking missing values\nprint(\"% of missing cabin info\")\nprint(round(train_data['Embarked'].isnull().sum()/train_data.shape[0]*100,2))\n\n#we'll it to most boarded port\nprint(train_data['Embarked'].value_counts())\n#here most boarded port is S\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#filling the missing values for age and Embarked and dropping the feature cabin\n\n#train_data1 = train_data.copy()\n#train_data1[\"Age\"].fillna(train_data[\"Age\"].median(skipna=True), inplace=True)\n#train_data1[\"Embarked\"].fillna(train_data['Embarked'].value_counts().idxmax(), inplace=True)\n#train_data1.drop('Cabin', axis=1, inplace=True)\n#train_data1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train_data\ntest=test_data\nfull_data = [train, test]\n\n# Some features of my own that I have added in\n# Gives the length of the name\ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n# Feature engineering steps taken from Sina\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n# Create new feature IsAlone from FamilySize\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n# Remove all NULLS in the Embarked column\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n# Create a New feature CategoricalAge\nfor dataset in full_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\n\nfor dataset in full_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n    \n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n    # Mapping Fare\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    # Mapping Age\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the data\ndataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp', 'Name_length', 'Has_Cabin','Parch']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()\n#test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_params = {\n    'kernel' : 'linear',\n    'C' : 0.025\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values # Creates an array of the train data\nx_test = test.values # Creats an array of the test data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(y_train)\n#print(x_train)\n#print(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Linear SVM\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\n\nY_pred_svm = linear_svc.predict(x_test)\n\nacc_linear_svc = round(linear_svc.score(x_train, y_train) * 100, 2)\nprint(\"linear SVM: \",acc_linear_svc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Gaussain Naive Bayes\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)  \nY_pred_gauss = gaussian.predict(x_test)  \nacc_gaussian = round(gaussian.score(x_train, y_train) * 100, 2)\nprint(\"Gaussain Naive Bayes: \",acc_gaussian)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\n\nY_pred_log = logreg.predict(x_test)\n\nacc_log = round(logreg.score(x_train, y_train) * 100, 2)\nprint(\"Logistic Regression: \", acc_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#K-Nearest Neighbours\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(x_train, y_train)  \nY_pred_knn = knn.predict(x_test)  \nacc_knn = round(knn.score(x_train, y_train) * 100, 2)\nprint(\"knn: \", acc_knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cross validation\nfrom sklearn.model_selection import cross_val_score\nkn = KNeighborsClassifier(n_neighbors = 3)\nscores = cross_val_score(kn, x_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(knn, x_train, y_train, cv=10)\nconfusion_matrix(y_train, predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mlxtend.plotting import plot_decision_regions\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom matplotlib.colors import ListedColormap\n\nX_train_reduced = PCA(n_components = 2).fit_transform(x_train)\nt = np.array(y_train)\nt = t.astype(np.integer)\nkf=KNeighborsClassifier(n_neighbors = 3)\nkf.fit(X_train_reduced,t)\nplt.figure(figsize = [15,10])\nplot_decision_regions(X_train_reduced, t, clf = kf, hide_spines = False, colors = 'purple,limegreen',\n                      markers = ['^','v'])\nplt.title('K-Nearst Neighbours')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_knn =KNeighborsClassifier(n_neighbors = 3)\nclf_knn.fit(x_train, y_train)  \ntest_df = test_data.drop(['Name','Has_Cabin','Ticket','Name_length','Cabin','SibSp','Parch'],axis = 1)\ntest_df.head()\n#Pclass\tSex\tAge\tFare\tEmbarked\tFamilySize\tIsAlone\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nptest = np.array(test_df)\npred = clf_knn.predict(x_test)\npredictions = pd.DataFrame(pred, index = test_df.index, columns = ['Survived'])\ndf= predictions['Survived']\nfinal=test_df['PassengerId']\nfinal=pd.concat([final,df],axis=1,join='inner')\nfinal.to_csv('predictions_knn.csv',index=False)\noutput=pd.read_csv(\"predictions_knn.csv\")\n\nfinal\n\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}